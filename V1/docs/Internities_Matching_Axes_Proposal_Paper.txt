

=== Page 1 ===

  
  1  
Proposal Paper  Solving the Radar Axis Standardization & Evidence-Weighted Matching Problem Date: January 07, 2026  
Executive Summary Internities‚Äô core product promise is that companies can describe an internship role and obtain a ‚ÄúCompany Radar‚Äù,students can describe themselves and obtain a ‚ÄúStudent Skill Radar‚Äù and the platform produces a high-accuracy, explainable match score.  The current gap: a radar chart is only meaningful if its axes are comparable. If every company and every student can invent axes freely, the platform risks low comparability, low accuracy and no trustworthy recommendations. If axes are fixed and rigid, the platform risks being too generic to capture what makes a role unique.  This paper proposes a hybrid solution that maximizes both comparability and flexibility: A set Axis library and custom skill radars for every role   The result is a system that can scale across role types, remain explainable, allow companies to express nuance, and allow students to prove claims‚Äîwhile keeping match scores calibrated and trustworthy.         


=== Page 2 ===

  
  2  
 
Contents Executive Summary ......................................................................................................................... 1 Contents .......................................................................................................................................... 2 1. The Axis Problem and Why It Matters ........................................................................................ 3 2. Design Principles for a Trustworthy Matching System ............................................................... 3 3. Proposal Part I ‚Äî Axis Standardization Without Losing Flexibility ............................................. 4 3.1 Recommended approach: Database axis library and different axes per role ....................... 4 3.2 What goes into axis library .................................................................................................... 4 3.3 How Companies Create a Company Radar in This Model ..................................................... 5 3.4 Axis Mapping: How to match when the student has a new skill radar for every role? ........ 5 4. Proposal Part II ‚Äî Evidence, Reasoning, and Confidence-Weighted Matching ......................... 5 4.1 Why ‚ÄúHard Scores Only‚Äù Will Not Produce the Matches we Want ...................................... 5 4.2 Proposed Axis Scoring Model: Score √ó Confidence ............................................................... 6 4.3 Evidence Types and ‚ÄúProof Upload‚Äù Design ......................................................................... 6 5. Matching Algorithm Blueprint .................................................................................................... 7 5.1 Multi-Stage Matching (Recommended) ................................................................................ 7 5.2 A Concrete Scoring Formula (Starting Point) ........................................................................ 7 5.4 Missing Data Handling .......................................................................................................... 8 6. Governance, Fairness, and Quality Control ................................................................................ 8 6.1 Axis Governance ................................................................................................................... 8 6.3 Anti-Gaming Measures ......................................................................................................... 8 7. Implementation Roadmap (MVP ‚Üí Production) ........................................................................ 9 7.1 MVP (Fast, Reliable, Explainable) .......................................................................................... 9 7.2 Post-MVP (Data-Driven Calibration and Model Improvement) ............................................ 9    


=== Page 3 ===

  
  3  
1. The Axis Problem and Why It Matters A radar chart is a visualization of a vector: each axis is a dimension, and the polygon shape communicates relative strengths/requirements. The quality of any match score derived from radars depends on whether the axes mean the same thing across profiles and if the single Axis scores are evidence based.  In practice, internship roles vary hugely. A ‚ÄúStrategy Consulting Intern‚Äù and a ‚ÄúBackend Engineering Intern‚Äù share very few relevant dimensions. Even within the same role family, companies emphasize different traits (e.g., ‚Äúclient readiness‚Äù vs ‚Äúanalytical rigor‚Äù). Our product needs enough flexibility to represent this nuance without destroying comparability.  There are several risk scenarios:  ‚Ä¢ If companies define arbitrary axes, Student Radar and Company Radar may not overlap ‚Üí matching becomes unreliable or impossible. ‚Ä¢ If matching relies only on axis name similarity, it becomes fragile (synonyms, phrasing, language, jargon).  ‚Ä¢ If matching requires exact axis equality, we force everyone into the same template ‚Üí roles become generic and users lose trust. ‚Ä¢ If weuse AI to infer axes on the fly without governance, we get non-repeatable match results and hard-to-explain behavior.  Therefore the platform needs an explicit strategy for: (a) defining possible axes, (b) mapping and normalizing axes, (c) scoring each axis in a way that is consistent, and (d) combining axis scores with qualitative reasoning and evidence.  
2. Design Principles for a Trustworthy Matching System Comparability: A ‚Äú7/10 in Python‚Äù should mean roughly the same across students and roles. Flexibility: Companies must be able to express role nuance (especially for top-tier internships). Explainability: Company Users should understand why a match is high/low at the axis level and overall. Evidence-first: Claims should be grounded in proofs (projects, grades, assessments, references). Calibration over aesthetics: The radar is a UI; the underlying model must be statistically stable and monitored.  Fairness and bias control: The system must be tested for prestige bias, demographic proxies, and other distortions. Human-in-the-loop: AI should assist; students and recruiters remain the decision makers. 


=== Page 4 ===

  
  4  
3. Proposal Part I ‚Äî Axis Standardization Without Losing Flexibility 3.1 Recommended approach: Database axis library and different axes per role  ‚Ä¢ We will be defining a set of (x) axes in our database which is not repetitive, applicable and is sufficient for all possible Industries we are targeting   ‚Ä¢ After filling out the questionnaire for the role creation, the AI builds a customized skill radar for this specific role. The company will then later be able to change the Axes with an assisting AI Chatbot under the constraint of the defined axis set.   ‚Ä¢ In the application process, the student has their set of skills and abilities which are then matched on every role individually. Technically, the AI builds a new skill radar for every role the student applies for by applying the skills and abilities the student has to the set role radar and matching both with each other.   3.2 What goes into axis library Treat axes as a product asset that we curate and version. The library should be: ‚Ä¢ Multi-type (hard skills, soft skills, work styles, motivations, constraints). ‚Ä¢ Role-family aware (consulting, finance, marketing, software engineering, data science, product, design‚Ä¶).  ‚Ä¢ Language Customized ‚Äì for every European market a different set of axes has to be defined  ‚Ä¢ Versioned (axes definitions can improve over time without breaking historical match scores). Each axis in the library is not just a name. It must include a structured definition package:  ‚Ä¢ Axis ID (stable, internal key) ‚Ä¢ Name + synonyms (for UI + matching) ‚Ä¢ Definition (what it measures, what it does NOT measure) ‚Ä¢ Scoring rubric (0‚Äì10 with anchors; e.g., 2, 5, 8 examples) ‚Ä¢ Evidence types that can support the axis (projects, grades, certificates, tests, recommendations‚Ä¶) ‚Ä¢ Assessment options (optional micro-tests that calibrate scoring) ‚Ä¢ Typical roles where the axis is used (helps suggest axes during role posting) 


=== Page 5 ===

  
  5  
3.3 How Companies Create a Company Radar in This Model A company should never start from a blank radar. Instead, during role posting, the AI proposes a ‚ÄúRole Template‚Äù based on role title, function, and description. The recruiter then edits: ‚Ä¢ Change the axes, if necessary, with the AI chatbot  ‚Ä¢ Set importance weights (e.g., 0‚Äì5) and/or mark as Must-Have / Nice-to-Have. ‚Ä¢ Provide a short rationale per axis (why it matters for success in the role).  ‚Ä¢ Edits minimum requirements (e.g. GPA of 8) which are evidence-based constraints the student must fulfill to even be matched on the rolls` skill radar 3.4 Axis Mapping: How to match when the student has a new skill radar for every role?  ‚Ä¢ Axis ID matching: when the student fills out the questionnaire and uploads his or her evidence, the axis score for each possible axis from the defined set is automatically created and evaluated based on the evidence. These Axis scores are checked minimum 2 times by different trained AI models. When applying to a new role, the subset of axes that the company used for their radar is not changed but automatically being used to create the student radar with the same axis subset. ‚Ä¢ Prompt-to-complete: for high-weight missing axes (a high weighted company axis that is not weighted on the student side, ask the student to add evidence before applying.  ‚Ä¢ Hard stop: if a must-have axis cannot be evaluated at all, mark as ‚ÄúNot enough data‚Äù and reduce match score (not a silent guess) - this prevents AI hallucinating  
4. Proposal Part II ‚Äî Evidence, Reasoning, and Confidence-Weighted Matching 4.1 Why ‚ÄúHard Scores Only‚Äù Will Not Produce the Matches we Want A single numeric score per axis is appealing, but it hides critical context:  ‚Ä¢ Two students can both score 7/10 in ‚ÄòLeadership‚Äô, but one has led a 20-person society with documented outcomes and the other only self-reports. ‚Ä¢ A student‚Äôs score may be correct but outdated (skill decay) or not relevant to the role context. ‚Ä¢ Some axes are inherently qualitative (‚Äòcommunication clarity‚Äô, ‚Äòstructured thinking‚Äô) and require narrative evidence. 


=== Page 6 ===

  
  6  
‚Ä¢ Recruiters care about the ‚Äúwhy‚Äù (how a student demonstrates a trait), not just the ‚Äúwhat‚Äù (a number). Therefore, the matching system must incorporate reasoning and proof in a structured way. The solution is to separate: (a) the axis score, (b) the confidence in that score, and (c) the evidence and narrative that justifies it. 4.2 Proposed Axis Scoring Model: Score √ó Confidence  For each axis, store multiple fields instead of one:  ‚Ä¢ Score (0‚Äì10): estimated capability/fit in that dimension. ‚Ä¢ Confidence (0‚Äì1): how certain the system is about the score. (verified docs > public portfolio > self-report) ‚Ä¢ Reasoning summary (short text): how the score was derived, explicitly referencing evidence.  4.3 Evidence Types and ‚ÄúProof Upload‚Äù Design To support an exclusive, high-signal marketplace, treat evidence as a first-class object in the product:  ‚Ä¢ Academic: Transcript, grades for relevant courses, honors, scholarships. ‚Ä¢ Portfolio: GitHub repos, project write-ups, design portfolios, research papers. ‚Ä¢ Work & internships: Offer letters, contracts, performance reviews, reference contacts. ‚Ä¢ Competitions & awards: Case competitions, hackathons, Olympiads, prizes. ‚Ä¢ Assessments: Assessments ‚Äì later in the Timeline (best option: strategic partnerships with Online assessment companies)  ‚Ä¢ Social proof: Verified recommendations or endorsements (structured, not free-text only). Verification should be progressive: ‚Ä¢ Unverified: user uploaded / linked. ‚Ä¢ Auto-verified: checksum/metadata, domain validation, basic fraud detection. ‚Ä¢ Institution-verified: e.g., university email, transcript check, or partner verification. ‚Ä¢ Company-verified: confirmed by employer during/after internship (powerful feedback signal).     


=== Page 7 ===

  
  7  
5. Matching Algorithm Blueprint 5.1 Multi-Stage Matching (Recommended) A robust system should not rely on one single model. Use a pipeline:  ‚Ä¢ Stage 0 - Hard filters: eligibility (visa/work authorization if relevant), start date, location constraints, language minimum, program year. ‚Ä¢ Stage 1 - Must-Have gates: if any must-have constraints < threshold OR axis is ‚Äòunknown‚Äô, reduce score sharply or mark as not eligible. ‚Ä¢ Stage 2 - Weighted axis score: compute a score using subset axes with confidence and evidence strength. ‚Ä¢ Stage 3 - Narrative alignment: compare the student‚Äôs axis reasoning + role axis rationale for consistency and relevance. ‚Ä¢ Stage 4 - Explainability: generate a shortlist explanation (top reasons, top gaps, and how to improve). 5.2 A Concrete Scoring Formula (Starting Point) We should implement a first-version scoring formula that is transparent and easy to back test:  For each axis i in the custom created student radar: ‚Ä¢ s_i = student score (0‚Äì10) ‚Ä¢ c_i = confidence (0‚Äì1)  Define an effective score for each axis:    eff_i = s_i * c_i   Then compute qualification score assuming there are 10 axes in the role: ‚àë!"".$%&.$'('()*.$'(‚Äà‚Äà         (where  ùëíùëìùëì.ùë†ùë°ùë¢.ùë†ùëêùëú‚Äà‚â§‚Äàùëêùëúùëöùëù.ùë†ùëêùëú	)   This formula will lead to a percentage of matching      


=== Page 8 ===

  
  8  
 5.4 Missing Data Handling Missing axes is unavoidable. The key is to make missingness explicit:  ‚Ä¢ Unknown axis score is not the same as low axis score. ‚Ä¢ For must-have axes, unknown should behave like ‚Äúnot eligible until assessed‚Äù. ‚Ä¢ For nice-to-have axes, unknown should reduce score slightly and trigger a prompt to add evidence.  ‚Ä¢ Give students fast ways to fill gaps: micro-assessments, quick evidence upload, or structured Q&A.  
6. Governance, Fairness, and Quality Control 6.1 Axis Governance Because axes define the language of our marketplace, we need governance from day 1:  ‚Ä¢ Axis owner: internal responsibility (product/ML) for the library. ‚Ä¢ Axis review cadence: monthly review of ‚Äúlibrary candidate‚Äù axes coming from companies. ‚Ä¢ Axis versioning: never change definitions silently; increment versions and maintain mapping for historical comparisons. ‚Ä¢ Deprecation policy: old axes can be hidden from UI but kept for historical match reproducibility.  6.3 Anti-Gaming Measures As soon as scoring affects opportunities, users will optimize it. Design for this:  ‚Ä¢ Separate ‚Äúself-reported‚Äù vs ‚Äúverified/assessed‚Äù contributions to axis scores. ‚Ä¢ Use confidence/evidence strength to down-weight unsupported claims. ‚Ä¢ Detect inconsistencies (e.g., claiming advanced Python but no evidence + failing micro-assessment). ‚Ä¢ Rate-limit sudden large score changes unless new evidence is added. ‚Ä¢ Keep rubric anchors visible (students know what 8/10 means, making the system feel fair). 


=== Page 9 ===

  
  9  
7. Implementation Roadmap (MVP ‚Üí Production) 7.1 MVP (Fast, Reliable, Explainable) For our MVP, prioritize stability and trust over model sophistication:  ‚Ä¢ Start with 40‚Äì80 canonical axes across 6‚Äì8 role families. ‚Ä¢ Role posting uses templates + limited axis customization. ‚Ä¢ Student onboarding generates initial axis scores using structured Q&A + CV parsing (optional). ‚Ä¢ Evidence objects + verification states exist from day 1 (even if verification is manual early). ‚Ä¢ Matching uses the transparent formula (Score √ó Confidence)  ‚Ä¢ Track feedback signals: apply ‚Üí interview ‚Üí offer ‚Üí performance (where available). 7.2 Post-MVP (Data-Driven Calibration and Model Improvement) After pilots, you can improve accuracy via learning loops:  ‚Ä¢ Calibrate axis scoring with outcomes data (which scores predict interviews/offers). ‚Ä¢ Refine weights per role family using historical hiring success patterns. ‚Ä¢ Expand axis library based on repeated extension requests. ‚Ä¢ Introduce assessments for high-stakes axes (coding, finance modeling, writing). ‚Ä¢ Build dashboards for companies showing how Internities reduced screening time and improved interview-to-offer conversion. 


=== Page 10 ===

  
  10  
 